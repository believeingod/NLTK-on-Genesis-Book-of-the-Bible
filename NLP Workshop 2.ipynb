{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how N-gram generator works in NLTK\n",
    "# Here are the Bigrams. \n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "sentence = input( \"Enter the sentence: \" )\n",
    "n = int(input( \"Enter the value of n: \" ))\n",
    "n_grams = ngrams(sentence.split(), n)\n",
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s try Trigrams N=3.\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "sentence = input( \"Enter the sentence: \" )\n",
    "n = int(input( \"Enter the value of n: \" ))\n",
    "n_grams = ngrams(sentence.split(), n)\n",
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about Quadrigram N=4? Let’s use the same sentence\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "sentence = input( \"Enter the sentence: \" )\n",
    "n = int(input( \"Enter the value of n: \" ))\n",
    "n_grams = ngrams(sentence.split(), n)\n",
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "# get rid of all the XML markup\n",
    "sentence = re.sub ('<.*>' , ' ', sentence)\n",
    "# get rid of punctuation (except periods!)\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "sentence = re.sub(punctuationNoPeriod, \"\", sentence)\n",
    "# first get individual words\n",
    "tokenized = sentence.split()\n",
    "# and get a list of all the bi-grams\n",
    "Bigrams = ngrams(tokenized, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate N-gram statistics, first import “collections” class \n",
    "# and invoke Counter() method over Bigrams to perform N-gram statistics analysis.\n",
    "import collections\n",
    "# get the frequency of each bigram in our corpus\n",
    "BigramFreq = collections.Counter(Bigrams)\n",
    "# what are the ten most popular ngrams in this corpus?\n",
    "BigramFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_para = \"\"\"To Sherlock Holmes she is always _the_ woman. I have seldom heard him mention her under any other name. \n",
    "In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. \n",
    "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. He was, I take it, the \n",
    "most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a\n",
    "false position. He never spoke of the softer passions, save with a gibe and a sneer. They were admirable things for the \n",
    "observer—excellent for drawing the veil from men’s motives and actions. But for the trained reasoner to admit such intrusions \n",
    "into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all \n",
    "his mental results. Grit in a sensitive instrument, or a crack in one of his own high-power lenses, would not be more disturbing \n",
    "than a strong emotion in a nature such as his. And yet there was but one woman to him, and that woman was the late Irene\n",
    "Adler, of dubious and questionable memory.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python script to remove punctuation marks and tokenize the first_para object\n",
    "import re, string\n",
    "# get rid of all the XML markup\n",
    "first_para = re.sub ('<.*>' , ' ', first_para)\n",
    "# get rid of punctuation (except periods!)\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "first_para = re.sub(punctuationNoPeriod, \"\", first_para)\n",
    "# first get individual words\n",
    "tokenized = first_para.split()\n",
    "# and get a list of all the bi-grams\n",
    "Bigrams = ngrams(tokenized, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Counter() method of Collections class to calculate Bigram statistics of first_para\n",
    "import collections\n",
    "# get the frequency of each bigram in our corpus\n",
    "BigramFreq = collections.Counter(Bigrams)\n",
    "# what are the ten most popular ngrams in this corpus?\n",
    "BigramFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load some sample books from the nltk databank\n",
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "from nltk.text import Text\n",
    "moby = Text(nltk.corpus.gutenberg.words( 'melville-moby_dick.txt' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby [1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Collections class and ngrams() method for Bigram statistics to identify top 20 most frequently bigrams occurred for the \n",
    "# entire Moby Dick literature.\n",
    "import collections\n",
    "# and get a list of all the bi-grams\n",
    "Bigrams = ngrams(moby, 2)\n",
    "# get the frequency of each bigram in our corpus\n",
    "BigramFreq = collections.Counter(Bigrams)\n",
    "# what are the 20 most popular ngrams in this corpus?\n",
    "BigramFreq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy Module \"en_core_web_sm\"\n",
    "import spacy\n",
    "nlp = spacy.load( \"en_core_web_sm\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Open and Read Text File \"Adventures_Holmes.txt\" Into file_handler \"fholmes\"\n",
    "fholmes = open( \"Adventures_Holmes.txt\", \"r\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Adventures of Sherlock Holmes\n",
    "holmes = fholmes.read()\n",
    "holmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Replace All Newline Symbols\n",
    "holmes = holmes.replace( \"\\n\", \" \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(holmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Invoke nlp() Method in spaCy\n",
    "nlp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes_doc =nlp(holmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Convert Text Document Into Sentence Object\n",
    "# SpaCy is practical for text document tokenization to convert text document object into (1) sentence objects and (2) tokens\n",
    "\n",
    "holmes_sentences = [sentence.text for sentence in holmes_doc.sents]\n",
    "holmes_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes_sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(holmes_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes_sentences[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Directly Tokenize Text Document\n",
    "holmes_words = [token.text for token in holmes_doc]\n",
    "holmes_words [130:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmes_words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (holmes_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_homles_tokens = nltk.word_tokenize(holmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_homles_tokens [104:153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for Efficiency Performance of the NLP Engine\n",
    "import nltk # or spacy\n",
    "import time\n",
    "start = time.time()\n",
    "#\n",
    "# YOUR NTLK or spaCy Tokenization codes\n",
    "#\n",
    "print( \"Time taken: %s s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
